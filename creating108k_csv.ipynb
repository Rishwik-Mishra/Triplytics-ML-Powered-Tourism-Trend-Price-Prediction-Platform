{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47188070-1a9b-4dd8-b536-c0fb9c077083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset from 'Railway_price_data.csv'...\n",
      "Original size: 326643 rows\n",
      "\n",
      "Identified the top 125 most important stations.\n",
      "New expanded size: 180839 rows\n",
      "\n",
      "Successfully created 'passenger_train_data_expanded.csv'.\n",
      "This file contains the new, expanded feature set for our model.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the name of your large input file\n",
    "input_filename = 'Railway_price_data.csv'\n",
    "output_filename = 'passenger_train_data_expanded.csv' # <-- New output name\n",
    "\n",
    "try:\n",
    "    # Load the full dataset\n",
    "    print(f\"Loading full dataset from '{input_filename}'...\")\n",
    "    df_full = pd.read_csv(input_filename)\n",
    "    print(f\"Original size: {len(df_full)} rows\")\n",
    "\n",
    "    # --- Find the most important stations ---\n",
    "    from_counts = df_full['fromStnCode'].value_counts()\n",
    "    to_counts = df_full['toStnCode'].value_counts()\n",
    "    all_station_counts = from_counts.add(to_counts, fill_value=0).sort_values(ascending=False)\n",
    "\n",
    "    # Get the list of the top 125 station codes\n",
    "    important_stations = all_station_counts.head(125).index.tolist()\n",
    "    print(f\"\\nIdentified the top {len(important_stations)} most important stations.\")\n",
    "\n",
    "    # --- THIS IS THE NEW LOGIC ---\n",
    "    # Keep rows where EITHER the origin OR the destination is in our Top 125 list\n",
    "    df_filtered = df_full[\n",
    "        (df_full['fromStnCode'].isin(important_stations)) | \n",
    "        (df_full['toStnCode'].isin(important_stations))\n",
    "    ]\n",
    "    print(f\"New expanded size: {len(df_filtered)} rows\") # This will be much larger\n",
    "\n",
    "    # --- Select only the columns we need ---\n",
    "    columns_to_keep = [\n",
    "        'fromStnCode', \n",
    "        'toStnCode', \n",
    "        'classCode', \n",
    "        'distance', \n",
    "        'duration', \n",
    "        'timeStamp',\n",
    "        'totalFare'\n",
    "    ]\n",
    "    \n",
    "    # Check for missing columns before proceeding\n",
    "    missing_cols = [col for col in columns_to_keep if col not in df_filtered.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: The original CSV is missing required columns: {missing_cols}\")\n",
    "    else:\n",
    "        df_final = df_filtered[columns_to_keep]\n",
    "\n",
    "        # --- Save the new, final file ---\n",
    "        df_final.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nSuccessfully created '{output_filename}'.\")\n",
    "        print(\"This file contains the new, expanded feature set for our model.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{input_filename}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d2682-3a3c-4ed0-b414-3e88be60a87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
